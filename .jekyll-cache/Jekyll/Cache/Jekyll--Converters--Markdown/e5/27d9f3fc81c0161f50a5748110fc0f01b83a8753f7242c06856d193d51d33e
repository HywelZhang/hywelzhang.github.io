I"<p>ç±»ï¼š org.apache.spark.SparkConf</p>

<h3 id="åŠŸèƒ½æ¦‚è¿°">åŠŸèƒ½æ¦‚è¿°</h3>
<ol>
  <li>é€šè¿‡key-valueé”®å€¼å¯¹æ¥è®¾ç½®Sparkç¨‹åºè¿è¡Œå‚æ•°</li>
  <li>å¤§å¤šæ•°æ—¶å€™ï¼Œä½¿ç”¨<code class="language-plaintext highlighter-rouge">new SparkConf()</code>æ¥æ–°å»ºï¼Œè¿™ä¸ªæ“ä½œè¿˜ä¼šåŠ è½½å¤–éƒ¨è®¾ç½®çš„spark.*çš„å±æ€§</li>
  <li>é€šè¿‡SparkConfåœ¨ç¨‹åºé‡Œç›´æ¥è®¾ç½®çš„å±æ€§ï¼Œä¼˜å…ˆçº§ä¼šé«˜äºç³»ç»Ÿè®¾ç½®</li>
  <li>åœ¨æµ‹è¯•æ—¶ï¼Œå¯ä»¥é€šè¿‡<code class="language-plaintext highlighter-rouge">new SparkConf(false)</code>æ¥è·³è¿‡åŠ è½½å¤–éƒ¨ç¯å¢ƒå±æ€§</li>
  <li>æ‰€æœ‰çš„setteræ–¹æ³•éƒ½æ”¯æŒé“¾å¼è°ƒç”¨ã€‚ä¾‹å¦‚ï¼Œ<code class="language-plaintext highlighter-rouge">new SparkConf().setMaster("local").setAppName("My app")</code></li>
</ol>

<h3 id="ä»£ç è§£æ">ä»£ç è§£æ</h3>

<p><strong>ä»¥ä¸‹ä»£ç ä»…ä¸ºåŸå§‹æ–‡ä»¶ä¸­éƒ¨åˆ†æºç ï¼Œä»…å¯¹æ–¹æ³•çº§åˆ«è¿›è¡Œåˆ†æ</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/**
 * @param loadDefaults æ˜¯å¦åŠ è½½å¤–éƒ¨å±æ€§è®¾ç½®ï¼Œé»˜è®¤ä¸ºture
 * @note ä¸€æ—¦å°†SparkConfå¯¹è±¡ä¼ é€’ç»™äº†sparkï¼ŒSparkConfä¼šå¤åˆ¶ç»™å„excutorï¼Œé‚£ä¹ˆå°±ä¸èƒ½å†ä¿®æ”¹å±æ€§å€¼ã€‚ä¹Ÿå°±æ˜¯è¯´sparkä¸æ”¯æŒè¿è¡Œæ—¶ä¿®æ”¹å‚æ•°
 */
class SparkConf(loadDefaults: Boolean) extends Cloneable with Logging with Serializable {
	//ä¿å­˜é”®å€¼å¯¹çš„HashMap
	private val settings = new ConcurrentHashMap[String, String]()	

	//åˆ›å»ºSparkConfï¼Œå¹¶é»˜è®¤åŠ è½½ç¯å¢ƒå˜é‡å±æ€§
 	def this() = this(true)
	
	//åŠ è½½ç¯å¢ƒå˜é‡
	//å‚æ•°silentï¼šæ˜¯å¦é™é»˜åŠ è½½ï¼Œé»˜è®¤fasleï¼Œä¼šæ£€æµ‹è¯¥keyæ˜¯å¦å·²åºŸå¼ƒ
	private[spark] def loadFromSystemProperties(silent: Boolean): SparkConf = {åŠ è½½ç¯å¢ƒå˜é‡ä¸­ä»¥'spark.'å¼€å¤´çš„å±æ€§ï¼Œå†è°ƒç”¨set(key, value, silent)æ·»åŠ åˆ°HashMapä¸­}

	//è®¾ç½®é”®å€¼å¯¹å±æ€§
	private[spark] def set(key: String, value: String, silent: Boolean): SparkConf = {åˆ¤æ–­keyå’Œvalueï¼Œå¦‚æœsilent==falseï¼Œæœ‰nullåˆ™æ‰“è­¦å‘Šï¼Œç„¶åsettings.put(key, value)}
	def set(key: String, value: String): SparkConf = {set(key, value, false)}
	
	//è®¾ç½®master
	def setMaster(master: String): SparkConf = {set("spark.master", master)}

	//è®¾ç½®AppName
	def setAppName(name: String): SparkConf = { set("spark.app.name", name)}

	//è®¾ç½®éœ€å¼•ç”¨çš„JaråŒ…ï¼Œè¯¥jaråŒ…è¿è¡Œæ—¶ä¼šå¤åˆ¶åˆ°é›†ç¾¤ä¸Š
	def setJars(jars: Seq[String]): SparkConf = {
      for (jar &lt;- jars if (jar == null)) logWarning("null jar passed to SparkContext constructor")
      set("spark.jars", jars.filter(_ != null).mkString(","))
    }
	def setJars(jars: Array[String]): SparkConf = {setJars(jars.toSeq)}

	//è®¾ç½®executorç¯å¢ƒå˜é‡
	def setExecutorEnv(variable: String, value: String): SparkConf = {set("spark.executorEnv." + variable, value)}
	def setExecutorEnv(variables: Seq[(String, String)]): SparkConf = {
      for ((k, v) &lt;- variables) {
        setExecutorEnv(k, v)
      }
      this
    }
	def setExecutorEnv(variables: Array[(String, String)]): SparkConf = {setExecutorEnv(variables.toSeq)}

	//è®¾ç½®spark Home
	def setSparkHome(home: String): SparkConf = {set("spark.home", home)}
	
	//è®¾ç½®å¤šä¸ªå±æ€§
	def setAll(settings: Traversable[(String, String)]): SparkConf = {
      settings.foreach { case (k, v) =&gt; set(k, v) }
      this
    }

	//è®¾ç½®æ—¶ï¼Œå¦‚æœå±æ€§ä¸å­˜åœ¨ï¼Œæ£€æµ‹æ˜¯å¦è¯¥å±æ€§å·²åºŸå¼ƒ
	def setIfMissing(key: String, value: String): SparkConf = {
      if (settings.putIfAbsent(key, value) == null) {logDeprecationWarning(key)}
      this
    }

	//ä½¿ç”¨kryoåºåˆ—åŒ–å’Œregisterå»å¤„ç†æŒ‡å®šclassï¼Œå¦‚æœè°ƒç”¨å¤šæ¬¡ï¼Œä¼šå°†classè¿½åŠ 
	def registerKryoClasses(classes: Array[Class[_]]): SparkConf = {...}

	//ä½¿ç”¨Kryo serializationå’Œregisteræ¥å¤„ç†ç»™å®šçš„Avroç»“æ„ï¼Œè¿™æ ·èƒ½å¤Ÿåºåˆ—åŒ–è®°å½•å¹¶æ˜¾è‘—å‡å°ç£ç›˜IO
	def registerAvroSchemas(schemas: Schema*): SparkConf = {...}

	//ç§»é™¤æŸä¸ªå±æ€§
	def remove(key: String): SparkConf = {settings.remove(key);this}

	//è·å–å±æ€§å€¼
	def get(key: String): String = {getOption(key).getOrElse(throw new NoSuchElementException(key))}

	//è¿”å›è¯¥Sparkåº”ç”¨ç¨‹åºçš„application IDï¼Œåœ¨TaskScheduleræ³¨å†ŒåŠExecutorå¯åŠ¨ä»¥åï¼Œdriverç«¯å¯ç”¨
	def getAppId: String = get("spark.app.id")

	//æ˜¯å¦åŒ…å«æŒ‡å®šå‚æ•°
	def contains(key: String): Boolean = {settings.containsKey(key) || configsWithAlternatives.get(key).toSeq.flatten.exists { alt =&gt; contains(alt.key) }}

	//éªŒè¯é…ç½®å‚æ•°æ˜¯å¦åˆæ³•
	private[spark] def validateSettings() {...}
	
	...
}

private[spark] object SparkConf extends Logging {
	//å·²åºŸå¼ƒçš„å±æ€§
	private val deprecatedConfigs: Map[String, DeprecatedConfig] = {...}
	//å½“å‰ç‰ˆæœ¬å¯ç”¨å±æ€§ï¼Œå¦‚æœå·²ç»åºŸå¼ƒåˆ™å¼¹å‡ºä¸€ä¸ªè­¦å‘Š
	private val configsWithAlternatives = Map[String, Seq[AlternateConfig]](...)	
	private val allAlternatives: Map[String, (String, AlternateConfig)] = {
	  configsWithAlternatives.keys.flatMap { key =&gt;
        configsWithAlternatives(key).map { cfg =&gt; (cfg.key -&gt; (key -&gt; cfg)) }
      }.toMap
	}
	
	//åˆ¤æ–­æ˜¯å¦executorè¿æ¥scheduleræ—¶æ‰€éœ€é…ç½®å±æ€§ï¼Œå‰©ä½™å±æ€§å°†æ˜¯driverç«¯å±æ€§
	def isExecutorStartupConf(name: String): Boolean = {å¼€å¤´æ˜¯"spark.auth"ä¸”é€šè¿‡å®‰å…¨éªŒè¯||"spark.ssl"||"spark.rpc"||"spark.network"}
	
	//å±æ€§æ˜¯å¦åŒ¹é…`spark.*.port` æˆ–è€… `spark.port.*`
	def isSparkPortConf(name: String): Boolean = {...}
	
	//éªŒè¯è¢«åºŸå¼ƒå±æ€§æ˜¯å¦å½“å‰ç‰ˆæœ¬å¯ç”¨
	def getDeprecatedConfig(key: String, conf: SparkConf): Option[String] = {...}

	//å¦‚æœå±æ€§å·²è¿‡æ—¶åºŸé™¤deprecatedï¼Œæ‰“ä¸€ä¸ªè­¦å‘Šæ—¥å¿—
	def logDeprecationWarning(key: String): Unit = {...}
	
	...
}
	
</code></pre></div></div>

<h3 id="æ€»ç»“">æ€»ç»“ï¼š</h3>
<p>æ ¸å¿ƒæ–¹æ³•</p>
<ol>
  <li>Setæ–¹æ³•ï¼ŒåŒæ—¶ä¼šè°ƒç”¨ä¸€ç³»åˆ—éªŒè¯æ–¹æ³•å»éªŒè¯å±æ€§æ˜¯å¦è¿‡æ—¶å¯ç”¨deprecated</li>
  <li>getï¼Œè·å–å±æ€§å€¼ï¼ŒåŒ…æ‹¬åŠ è½½è¿›æ¥çš„ç³»ç»Ÿè®¾ç½®</li>
</ol>
:ET